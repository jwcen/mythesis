{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JKT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "from tqdm import notebook, tqdm\n",
    "from scipy.sparse import coo_matrix\n",
    "from functools import partial\n",
    "# from coo import co_acc_probs, co_acc_skills\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiprocessing to speedup matrix extraction\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "pool = multiprocessing.Pool(processes=cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess data csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess assist2009\n",
    "read_col = ['order_id', 'assignment_id', 'user_id', 'assistment_id', 'problem_id', 'correct', \n",
    "            'sequence_id', 'base_sequence_id', 'skill_id', 'skill_name', 'original']\n",
    "target = 'correct'\n",
    "\n",
    "# read in the data\n",
    "df = pd.read_csv('./data/a09/skill_builder_data.csv', low_memory=False, encoding=\"ISO-8859-1\")[read_col]\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing empty skill_id, records number 459208\n",
      "After removing scaffolding problems, records number 433161\n",
      "deleted user number based min-inters 290\n",
      "After deleting some users, records number 432702\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# delete empty skill_id\n",
    "df = df.dropna(subset=['skill_id'])\n",
    "df = df[~df['skill_id'].isin(['noskill'])]\n",
    "df.skill_id = df.skill_id.astype('int')\n",
    "print('After removing empty skill_id, records number %d' % len(df))\n",
    "\n",
    "# delete scaffolding problems\n",
    "df = df[df['original'].isin([1])]\n",
    "print('After removing scaffolding problems, records number %d' % len(df))\n",
    "\n",
    "min_inter_num = 3\n",
    "users = df.groupby(['user_id'], as_index=True)\n",
    "delete_users = []\n",
    "for u in users:\n",
    "    if len(u[1]) < min_inter_num:\n",
    "        delete_users.append(u[0])\n",
    "print('deleted user number based min-inters %d' % len(delete_users))\n",
    "df = df[~df['user_id'].isin(delete_users)]\n",
    "df = df[[ 'user_id', 'problem_id', 'skill_id', 'correct']]\n",
    "print('After deleting some users, records number %d' % len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 5,\"[55969, 55970, ...\n",
    "skill_df = df[['skill_id', 'problem_id']].groupby(['skill_id'], as_index=True).apply(lambda r: np.array(list(set(r['problem_id'].values))))\n",
    "# joblib.dump(skill_df, 'Data/assist2009/skill_prob.pkl.zip')\n",
    "user_prob = df[['user_id', 'problem_id', 'correct']].groupby(['user_id', 'problem_id'])['correct'].agg('mean')\n",
    "# print(user_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract concept-to-concept matrix and exercise -to-exercise matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_acc_probs(user_prob, prob1, prob2):\n",
    "    count = 0\n",
    "    agg = 0\n",
    "    for user, prob in user_prob.index:\n",
    "        if prob == prob1:\n",
    "            if prob2 in user_prob[user].index:\n",
    "                count += 1\n",
    "                agg += user_prob[user][prob1] * user_prob[user][prob2]\n",
    "#                 print('user {} answered two questions, with {} answered {}, and {} answered {}'\n",
    "#                   .format(user, prob1, user_prob[user][prob1], prob2, user_prob[user][prob2]))\n",
    "    if count == 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return agg/count\n",
    "\n",
    "\n",
    "def co_acc_skills(user_skill, skill1, skill2):\n",
    "    count = 0\n",
    "    agg = 0\n",
    "    for user, skill in user_skill.index:\n",
    "        if skill1 == skill: \n",
    "            if skill2 in user_skill[user].index:\n",
    "                count += 1\n",
    "                agg += user_skill[user][skill1] * user_skill[user][skill2]\n",
    "    if count == 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return agg/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e2e matrix\n",
    "skill_mats = []\n",
    "e2e = partial(co_acc_probs, user_prob)\n",
    "# e2e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for skill in tqdm(skill_df.index):\n",
    "    print('processing skill: ', skill)\n",
    "    skill_probs = skill_df[skill]\n",
    "    row = []\n",
    "    col = []\n",
    "    args = ((prob1, prob2) for i, prob1 in enumerate(skill_probs) for prob2 in skill_probs[i:])\n",
    "    val = pool.starmap(e2e, args)\n",
    "    print('matrix size is: ', len(val))\n",
    "    for i in range(len(skill_probs)):\n",
    "        for j in range(i, len(skill_probs)):\n",
    "            row.append(i)\n",
    "            col.append(j)\n",
    "    assert(len(val)==len(row))\n",
    "    mat = coo_matrix((val, (row, col)), shape=(len(skill_probs), len(skill_probs)))\n",
    "    skill_mats.append(mat)\n",
    "# joblib.dump(skill_mats, 'para_e2e.pkl.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_acc = df[['skill_id', 'correct']].groupby('skill_id')['correct'].agg('mean')\n",
    "user_skill = df[['user_id', 'skill_id', 'correct']].groupby(['user_id', 'skill_id'])['correct'].agg('mean')\n",
    "\n",
    "for user, skill in tqdm(user_skill.index):\n",
    "    if user_skill[user][skill]>=skill_acc[skill]:\n",
    "        user_skill[user][skill] = 1\n",
    "    else:\n",
    "        user_skill[user][skill] = 0\n",
    "        \n",
    "# c2c matrix\n",
    "row = []\n",
    "col = []\n",
    "val = []\n",
    "print('Extracting c2c matrix...')\n",
    "arg_skills = ((skill1, skill2) for (i, skill1) in enumerate(skill_df.index) for skill2 in skill_df.index[i:])\n",
    "c2c = partial(co_acc_skills, user_skill)\n",
    "val = pool.starmap(c2c, arg_skills)\n",
    "for i in range(len(skill_df.index)):\n",
    "    for j in range(i, len(skill_df.index)):\n",
    "        row.append(i)\n",
    "        col.append(j)\n",
    "mat_skill = coo_matrix((val, (row, col)), shape=(len(skill_df.index), len(skill_df.index)))\n",
    "joblib.dump(mat_skill,'c2c.pkl.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training matrix in Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# import torch_geometric.nn as pyg_nn\n",
    "# import torch_geometric.utils as pyg_utils\n",
    "\n",
    "import time, joblib\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2c = joblib.load('./data/test/c2c.pkl.zip')\n",
    "skill_df = joblib.load('./data/test/skill_prob.pkl.zip')\n",
    "c2c_t = c2c.transpose()\n",
    "c2c_t.setdiag(0)\n",
    "c2c_add = c2c+c2c_t\n",
    "# c2c_add.setdiag(0)\n",
    "c2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = pyg_nn.GCNConv(in_channels, 2 * out_channels, cached=True)\n",
    "        self.conv2 = pyg_nn.GCNConv(2 * out_channels, out_channels, cached=True)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = (self.conv1(x, edge_index))\n",
    "        self.feature = self.conv2(x, edge_index)\n",
    "        return self.feature\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(x, train_pos_edge_index)\n",
    "    loss = model.recon_loss(z, train_pos_edge_index)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "#     writer.add_scalar(\"loss\", loss.item(), epoch)\n",
    "\n",
    "def test(pos_edge_index, neg_edge_index):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(x, train_pos_edge_index)\n",
    "    return model.test(z, pos_edge_index, neg_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train c2c embedding\n",
    "x = torch.tensor(c2c_add.toarray().tolist(), dtype=torch.float)\n",
    "edge_index, edge_weight = pyg_utils.convert.from_scipy_sparse_matrix(c2c_add)\n",
    "data = Data(edge_index=edge_index, x=x)\n",
    "\n",
    "channels = 10\n",
    "dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('CUDA availability:', torch.cuda.is_available())\n",
    "\n",
    "model = pyg_nn.GAE(Encoder(123, channels)).to(dev)\n",
    "labels = data.y\n",
    "data.train_mask = data.val_mask = data.test_mask = data.y = None\n",
    "data = train_test_split_edges(data, val_ratio=0, test_ratio=0.2)\n",
    "\n",
    "x, train_pos_edge_index = data.x.to(dev), data.train_pos_edge_index.to(dev)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "\n",
    "for epoch in range(1, 2001):\n",
    "    train(epoch)\n",
    "    auc, ap = test(data.test_pos_edge_index, data.test_neg_edge_index)\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, auc, ap))\n",
    "c2c_emb = {}\n",
    "for i, emb in enumerate(model.encoder.feature.cpu()):\n",
    "    c2c_emb[skill_df.index[i]] = emb\n",
    "joblib.dump(c2c_emb, 'Data/assist2009/c2c_emb.pkl.zip') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train e2e embedding\n",
    "e2e_mat = joblib.load('Data/assist2009/para_e2e.pkl.zip')\n",
    "e2e_emb = {}\n",
    "channels = 10\n",
    "for i, e2e in enumerate(e2e_mat):\n",
    "    skill_id = skill_df.index[i]\n",
    "    if len(skill_df[skill_id]) >= channels:\n",
    "        \n",
    "        print('processing questions in skill: {} ======'.format(skill_id))\n",
    "        e2e_t = e2e.transpose()\n",
    "        e2e_t.setdiag(0)\n",
    "        e2e_add = e2e + e2e_t\n",
    "\n",
    "        x = torch.tensor(e2e_add.toarray().tolist(), dtype=torch.float)\n",
    "        edge_index, edge_weight = pyg_utils.convert.from_scipy_sparse_matrix(e2e_add)\n",
    "        data = Data(edge_index=edge_index, x=x)\n",
    "\n",
    "        # encoder: written by us; decoder: default (inner product)\n",
    "        model = pyg_nn.GAE(Encoder(e2e.shape[0], channels)).to(dev)\n",
    "        # model = pyg_nn.GAE(Encoder(dataset.num_features, channels)).to(dev)\n",
    "\n",
    "        labels = data.y\n",
    "        data.train_mask = data.val_mask = data.test_mask = data.y = None\n",
    "        # data = model.split_edges(data)\n",
    "\n",
    "        data = train_test_split_edges(data, val_ratio=0, test_ratio=0.2)\n",
    "\n",
    "        x, train_pos_edge_index = data.x.to(dev), data.train_pos_edge_index.to(dev)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "        for epoch in range(1, 2001):\n",
    "            train(epoch)\n",
    "            auc, ap = test(data.test_pos_edge_index, data.test_neg_edge_index)\n",
    "            # writer.add_scalar(\"AUC\", auc, epoch)\n",
    "            # writer.add_scalar(\"AP\", ap, epoch)\n",
    "            if epoch % 500 == 0:\n",
    "                print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, auc, ap))\n",
    "        e2e_emb[skill_id] = model.encoder.feature.cpu()\n",
    "joblib.dump(e2e_emb, 'Data/assist2009/e2e_emb.pkl.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding + DKT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cenjw/anaconda3/envs/kt/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "e2e_emb = joblib.load('./data/a09/e2e_emb.pkl.zip')\n",
    "c2c_emb = joblib.load('./data/a09/c2c_emb.pkl.zip')\n",
    "skill_prob = joblib.load('./data/a09/skill_prob.pkl.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_skill_prob = {}\n",
    "channel = 10\n",
    "for i, skill_id in enumerate(skill_prob.index):\n",
    "    if len(skill_prob[skill_id])>= channel:\n",
    "        filtered_skill_prob[skill_id] = skill_prob[skill_id]\n",
    "# joblib.dump(filtered_skill_prob, 'Data/assist2009/filtered_skill_prob.pkl.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization\n",
    "scaler = StandardScaler()\n",
    "all_c_v = []\n",
    "for k,v in c2c_emb.items():\n",
    "    all_c_v.extend(list(v.numpy()))\n",
    "all_c_v = scaler.fit_transform(np.array(all_c_v).reshape(-1,1))\n",
    "all_c_v1 = {}\n",
    "for i, (k,v) in enumerate(c2c_emb.items()):\n",
    "    all_c_v1[k] = all_c_v[i*10:(i+1)*10].reshape(-1,)\n",
    "all_e_v = {}\n",
    "for skill,qu_embs in e2e_emb.items():\n",
    "    q_num = qu_embs.shape[0]\n",
    "    temp_all_v = qu_embs.numpy().reshape(-1,)\n",
    "    temp_all_v = scaler.fit_transform(np.array(temp_all_v).reshape(-1,1))\n",
    "    all_e_v[skill] = temp_all_v.reshape(-1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112/112 [00:00<00:00, 15378.34it/s]\n",
      "100%|██████████| 112/112 [00:00<00:00, 731.16it/s]\n",
      "100%|██████████| 15879/15879 [00:00<00:00, 410430.35it/s]\n"
     ]
    }
   ],
   "source": [
    "skill_emb = {}\n",
    "for skill in tqdm(filtered_skill_prob.keys()):\n",
    "    temp_c = (np.array(all_c_v1[skill]))\n",
    "    temp_e = np.array(np.mean(all_e_v[skill], axis=0))\n",
    "    skill_emb[skill] = np.append(temp_c, temp_e)\n",
    "prob_emb = {}\n",
    "for skill in tqdm(filtered_skill_prob.keys()):\n",
    "    for i, prob in enumerate(filtered_skill_prob[skill]):\n",
    "        temp_c = (np.array(all_c_v1[skill]))\n",
    "        temp_e = (np.array(all_e_v[skill][i]))\n",
    "        new_emb = np.append(temp_c, temp_e)\n",
    "        if prob in prob_emb.keys():\n",
    "            prob_emb[prob] = np.row_stack((prob_emb[prob], new_emb)).squeeze()\n",
    "#             print(prob_emb[prob].shape)\n",
    "        else: prob_emb[prob] = new_emb\n",
    "for prob in tqdm(prob_emb.keys()):\n",
    "    if len(prob_emb[prob].shape) > 1:\n",
    "        prob_emb[prob] = np.mean(prob_emb[prob], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test data\n",
    "read_col = ['order_id', 'assignment_id', 'user_id', 'assistment_id', 'problem_id', 'correct', \n",
    "            'sequence_id', 'base_sequence_id', 'skill_id', 'skill_name', 'original']\n",
    "target = 'correct'\n",
    "# read in the data\n",
    "# df = pd.read_csv('./data/a09/skill_builder_data.csv', low_memory=False, encoding=\"ISO-8859-1\")[read_col]\n",
    "# df = df.sort_values(['order_id', 'user_id'])\n",
    "# # delete empty skill_id\n",
    "# df = df.dropna(subset=['skill_id'])\n",
    "# df = df[~df['skill_id'].isin(['noskill'])]\n",
    "# df.skill_id = df.skill_id.astype('int')\n",
    "# print('After removing empty skill_id, records number %d' % len(df))\n",
    "\n",
    "# # delete scaffolding problems\n",
    "# df = df[df['original'].isin([1])]\n",
    "# print('After removing scaffolding problems, records number %d' % len(df))\n",
    "\n",
    "# #delete the users whose interaction number is less than min_inter_num\n",
    "# min_inter_num = 3\n",
    "# users = df.groupby(['user_id'], as_index=True)\n",
    "# delete_users = []\n",
    "# for u in users:\n",
    "#     if len(u[1]) < min_inter_num:\n",
    "#         delete_users.append(u[0])\n",
    "# print('deleted user number based min-inters %d' % len(delete_users))\n",
    "# df = df[~df['user_id'].isin(delete_users)]\n",
    "# df = df[['user_id', 'problem_id', 'skill_id', 'correct']]\n",
    "# print('After deleting some users, records number %d' % len(df))\n",
    "# # print('features: ', df['assistment_id'].unique(), df['answer_type'].unique())\n",
    "\n",
    "# df = df[df['skill_id'].isin(filtered_skill_prob.keys())]\n",
    "# df['skill_cat'] = df['skill_id'].astype('category').cat.codes\n",
    "# df['e_emb'] = df['problem_id'].apply(lambda r: prob_emb[r])\n",
    "# df['c_emb'] = df['skill_id'].apply(lambda r: skill_emb[r])\n",
    "\n",
    "df = pd.read_csv('./to_result.csv')\n",
    "\n",
    "group_c = df[['user_id', 'c_emb', 'correct']].groupby('user_id').apply(lambda r: (np.array(r['c_emb'].tolist()).squeeze(), r['correct'].values))\n",
    "train_group_c = group_c.sample(frac=0.8, random_state=2020)\n",
    "test_group_c = group_c[~group_c.index.isin(train_group_c.index)]\n",
    "# joblib.dump(train_group_c, 'Data/assist2009/train_group_c.pkl.zip')\n",
    "# joblib.dump(test_group_c, 'Data/assist2009/test_group_c.pkl.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DKT Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class DKTDataset(Dataset):\n",
    "    def __init__(self, group, min_samples=3, max_seq=100):\n",
    "        super(DKTDataset, self).__init__()\n",
    "        self.max_seq = max_seq\n",
    "        self.samples = {}\n",
    "        \n",
    "        self.user_ids = []\n",
    "        for user_id in group.index:\n",
    "            q, qa = group[user_id]\n",
    "            if len(q) < min_samples:\n",
    "                continue\n",
    "            \n",
    "            # Main Contribution\n",
    "            if len(q) > self.max_seq:\n",
    "                total_questions = len(q)\n",
    "                initial = total_questions % self.max_seq\n",
    "                if initial >= min_samples:\n",
    "                    self.user_ids.append(f\"{user_id}_0\")\n",
    "                    self.samples[f\"{user_id}_0\"] = (q[:initial], qa[:initial])\n",
    "                for seq in range(total_questions // self.max_seq):\n",
    "                    self.user_ids.append(f\"{user_id}_{seq+1}\")\n",
    "                    start = initial + seq * self.max_seq\n",
    "                    end = start + self.max_seq\n",
    "                    self.samples[f\"{user_id}_{seq+1}\"] = (q[start:end], qa[start:end])\n",
    "            else:\n",
    "                user_id = str(user_id)\n",
    "                self.user_ids.append(user_id)\n",
    "                self.samples[user_id] = (q, qa)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.user_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        user_id = self.user_ids[index]\n",
    "        q_, qa_ = self.samples[user_id]\n",
    "        seq_len = len(q_)\n",
    "\n",
    "        q = np.zeros((self.max_seq, q_.shape[1]))\n",
    "        qa = np.zeros(self.max_seq, dtype=int)\n",
    "        if seq_len == self.max_seq:\n",
    "            q[:] = q_\n",
    "            qa[:] = qa_\n",
    "        else:\n",
    "            q[-seq_len:] = q_\n",
    "            qa[-seq_len:] = qa_\n",
    "        \n",
    "        x_emb = self.onehot(q[:-1], qa[:-1])\n",
    "        q_next = q[1:]\n",
    "        labels = qa[1:]\n",
    "        \n",
    "        return x_emb, q_next, labels\n",
    "\n",
    "    \n",
    "    def onehot(self, questions, answers):\n",
    "        emb_num = questions.shape[-1]\n",
    "        result = np.zeros(shape=[self.max_seq-1, 2*emb_num])\n",
    "        for i in range(self.max_seq-1):\n",
    "            if answers[i] > 0:\n",
    "                result[i][:emb_num] = questions[i]\n",
    "            elif answers[i] == 0:\n",
    "                result[i][emb_num:] = questions[i]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch\n",
    "from torch import nn\n",
    "class DKT(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, device):\n",
    "        super(DKT, self).__init__()\n",
    "        self.device = device\n",
    "        # Number of hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "        \n",
    "        # RNN\n",
    "        self.rnn = nn.RNN(input_dim*2, hidden_dim, layer_dim, batch_first=True, nonlinearity='tanh')\n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim+input_dim, output_dim)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, q_next):\n",
    "        \n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)).to(self.device)\n",
    "            \n",
    "        # One time step\n",
    "        out_next, hn = self.rnn(x, h0)\n",
    "        out_next = torch.cat((out_next, q_next), axis=2)\n",
    "#         out_next = torch.cat((out.reshape(-1,hidden_dim), q_next.reshape(-1,20)), axis=1)\n",
    "        out = (self.fc(out_next))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, dataloader, optimizer, criterion, scheduler=None,  device=\"cpu\"):\n",
    "    model.train()\n",
    "\n",
    "    train_loss = []\n",
    "    num_corrects = 0\n",
    "    num_total = 0\n",
    "    labels = []\n",
    "    outs = []\n",
    "\n",
    "    for x_emb, q_next, y in (dataloader):\n",
    "        x = x_emb.to(device).float()\n",
    "        y = y.to(device).float()\n",
    "        q_next = q_next.to(device).float()\n",
    "        \n",
    "        out = model(x, q_next).squeeze()#[:, :-1]\n",
    "        \n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         scheduler.step()\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "        target_mask = (q_next!=0).unique(dim=2).squeeze()\n",
    "#         target_mask = (y!=-1)\n",
    "    \n",
    "        filtered_out = torch.masked_select(out, target_mask)\n",
    "        filtered_label = torch.masked_select(y, target_mask)\n",
    "        filtered_pred = (torch.sigmoid(filtered_out) >= 0.5).long()\n",
    "        \n",
    "        num_corrects += (filtered_pred == filtered_label).sum().item()\n",
    "        num_total += len(filtered_label)\n",
    "\n",
    "        labels.extend(filtered_label.view(-1).data.cpu().numpy())\n",
    "        outs.extend(filtered_pred.view(-1).data.cpu().numpy())\n",
    "\n",
    "    acc = num_corrects / num_total\n",
    "    auc = roc_auc_score(labels, outs)\n",
    "    loss = np.mean(train_loss)\n",
    "\n",
    "    return loss, acc, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_fn(model, dataloader, criterion, device=\"cpu\"):\n",
    "    model.eval()\n",
    "\n",
    "    valid_loss = []\n",
    "    num_corrects = 0\n",
    "    num_total = 0\n",
    "    labels = []\n",
    "    outs = []\n",
    "\n",
    "    for x_emb, q_next, y in (dataloader):\n",
    "        x = x_emb.to(device).float()\n",
    "        y = y.to(device).float()\n",
    "        q_next = q_next.to(device).float()\n",
    "        out = model(x, q_next).squeeze()#[:, :-1]\n",
    "        \n",
    "        loss = criterion(out, y)\n",
    "        valid_loss.append(loss.item())\n",
    "        \n",
    "        target_mask = (q_next!=0).unique(dim=2).squeeze()\n",
    "#         target_mask = (y!=-1)\n",
    "    \n",
    "        filtered_out = torch.masked_select(out, target_mask)\n",
    "        filtered_label = torch.masked_select(y, target_mask)\n",
    "        filtered_pred = (torch.sigmoid(filtered_out) >= 0.5).long()\n",
    "        \n",
    "        num_corrects += (filtered_pred == filtered_label).sum().item()\n",
    "        num_total += len(filtered_label)\n",
    "\n",
    "        labels.extend(filtered_label.view(-1).data.cpu().numpy())\n",
    "        outs.extend(filtered_pred.view(-1).data.cpu().numpy())\n",
    "\n",
    "    acc = num_corrects / num_total\n",
    "    auc = roc_auc_score(labels, outs)\n",
    "    pre = precision_score(labels, outs)\n",
    "    f1 = f1_score(labels, outs)\n",
    "    rec = recall_score(labels, outs)\n",
    "    loss = np.mean(valid_loss)\n",
    "\n",
    "    return loss, acc, pre, rec, f1, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings\n",
    "input_dim = 20    # input dimension\n",
    "hidden_dim = 128  # hidden layer dimension\n",
    "layer_dim = 1     # number of hidden layers\n",
    "output_dim = 1   # output dimension\n",
    "MAX_LEARNING_RATE = 1e-3\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 128\n",
    "MAX_SEQ = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataset = DKTDataset(train_group_c, max_seq=MAX_SEQ)\n",
    "train_dataloader  = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_dataset = DKTDataset(test_group_c, max_seq=MAX_SEQ)\n",
    "valid_dataloader  = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCEWithLogitsLoss()"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')#(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = DKT(input_dim, hidden_dim, layer_dim, output_dim, device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=MAX_LEARNING_RATE, steps_per_epoch=len(train_dataloader), epochs=EPOCHS\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_234496/2481887013.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#     print(\"epoch - {}/{} train: - {:.3f} acc - {:.3f} auc - {:.3f}\".format(epoch+1, EPOCHS, loss, acc, auc))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_234496/765043131.py\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(model, dataloader, optimizer, criterion, scheduler, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kt/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kt/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kt/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kt/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_234496/2821971533.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mqa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mseq_len\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "for epoch in (range(EPOCHS)):\n",
    "    loss, acc, auc = train_fn(model, train_dataloader, optimizer, criterion, device)\n",
    "#     print(\"epoch - {}/{} train: - {:.3f} acc - {:.3f} auc - {:.3f}\".format(epoch+1, EPOCHS, loss, acc, auc))\n",
    "    loss, acc, pre, rec, f1, auc = valid_fn(model, valid_dataloader, criterion, device)\n",
    "\n",
    "    res = \"epoch - {}/{} valid: - {:.3f} acc - {:.3f} pre - {:.3f} rec - {:.3f} f1 - {:3f} auc - {:.3f}\".format(epoch+1, EPOCHS, loss, acc, pre, rec, f1, auc)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_skill = 100\n",
    "viz_data = []\n",
    "ran = np.random.randint(low=0, high=112, size=num_skill)\n",
    "for i in ran:\n",
    "    c_em = skill_emb[list(skill_emb.keys())[i]]\n",
    "    viz_data.append(c_em.reshape(-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qu_len = []\n",
    "for i, k in enumerate(skill_emb.keys()):\n",
    "    if i<num_skill:\n",
    "        questions = skill_prob[k]\n",
    "        qu_len.append(len(questions))\n",
    "        q_emb = []\n",
    "        for j, q in enumerate(questions):\n",
    "            if j<100:\n",
    "                q_emb.append((prob_emb[q].reshape(-1,)))\n",
    "#         q_emb = scaler.fit_transform(q_emb)\n",
    "        viz_data.extend(q_emb)\n",
    "viz_data = scaler.fit_transform(viz_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['red' for i in range(num_skill)] + ['blue' for i in range(num_skill, len(viz_data))]\n",
    "s = [50 for i in range(num_skill)] + [1 for j in range(num_skill,len(viz_data))]\n",
    "xs, ys = zip(*TSNE(n_components=2, init='random', perplexity=30).fit_transform(viz_data))\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(xs, ys, color=colors, s=s)\n",
    "plt.savefig('cluster.eps', format='eps')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
